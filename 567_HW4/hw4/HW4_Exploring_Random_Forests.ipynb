{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vse0exAHic_R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support as prfs\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load MNIST Dataset\n",
        "Create two classes (label 0 for digits 0-4 and label 1 for digits 5-9), and get the train and test set."
      ],
      "metadata": {
        "id": "slABZMo_79c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(n_samples = 10000):\n",
        "  '''\n",
        "  Input: n_samples is the total number of samples processed; both train and test data will have n_samples//2 samples.\n",
        "\n",
        "  Outputs: X_tr, Y_tr are the training images and corresponding (binary) labels; X_te, Y_te are the test images and labels\n",
        "  '''\n",
        "  mnist_tr = datasets.MNIST(root = './data', train = True, download = True, transform = transforms.Compose([\n",
        "                               transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]))\n",
        "  X_tr = []\n",
        "  Y_tr = []\n",
        "  X_te = []\n",
        "  Y_te = []\n",
        "  \n",
        "  for idx, (im, label) in enumerate(mnist_tr):\n",
        "      if idx == n_samples:\n",
        "        print(f'Converting image {idx}/{len(mnist_tr)}')\n",
        "        break\n",
        "      im_array = np.array(im) \n",
        "      binary_label = 0 if label < 5 else 1\n",
        "\n",
        "      if idx < n_samples//2:\n",
        "        X_tr.append(im_array)\n",
        "        Y_tr.append(binary_label)\n",
        "      else:\n",
        "        X_te.append(im_array)\n",
        "        Y_te.append(binary_label)\n",
        "\n",
        "  X_tr = np.asarray(X_tr)\n",
        "  Y_tr = np.asarray(Y_tr)\n",
        "  X_te = np.asarray(X_te)\n",
        "  Y_te = np.asarray(Y_te)\n",
        "\n",
        "  return X_tr, X_te, Y_tr, Y_te"
      ],
      "metadata": {
        "id": "9WqPnJB8rICf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test a Tree-based Model"
      ],
      "metadata": {
        "id": "oOzdW3R3dPSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, t_train, max_depth = 10, min_samples_leaf = 10, criterion = 'gini', \n",
        "                seed = 60, model_type = 'DT', n_estimators = 50, max_samples = 0.5, max_features = 100):\n",
        "  '''\n",
        "  Inputs: \n",
        "    X_train, t_train: Training samples and labels\n",
        "    max_depth (int): The maximum depth of the decision tree.\n",
        "    min_samples_leaf (int): The minimum number of samples required to be at a leaf node.\n",
        "    criterion (string): The function to measure the quality of a split, \n",
        "                        can be 'gini' for the Gini impurity, 'log_loss' and 'entropy', both for the information gain\n",
        "    seed (int): Controls the randomness of the estimator. \n",
        "    model_type (string): The type of model being used, can be 'DT' for decision tree, \n",
        "                         'RF' for random forest, 'DT_Bagging' for decision tree with bagging and 'AdaBoost' for boosting. \n",
        "    n_estimators (int): The number of decision trees in the forest.\n",
        "    max_samples (int or float): The number of samples to draw from the training set to train each decision tree in the forest.\n",
        "    max_features (int or float): The number of features to consider when looking for the best split. \n",
        "\n",
        "  Output:\n",
        "    model: The model with the specified parameters, trained on the input data.\n",
        "  '''\n",
        "  #Decision Tree\n",
        "  if model_type == 'DT':\n",
        "    model = tree.DecisionTreeClassifier(criterion = criterion,\n",
        "            random_state = seed, max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n",
        "  \n",
        "  #Random Forest\n",
        "  elif model_type == 'RF':\n",
        "    model = RandomForestClassifier(n_estimators = n_estimators, criterion = criterion,\n",
        "                                   max_depth = max_depth, min_samples_leaf = min_samples_leaf, \n",
        "                                   random_state = seed, max_samples = max_samples, max_features = max_features)\n",
        "  \n",
        "  #Decision Tree with Bagging\n",
        "  elif model_type == 'DT_Bagging':\n",
        "    model1 = tree.DecisionTreeClassifier(criterion = criterion,\n",
        "            random_state = seed, max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n",
        "    model = BaggingClassifier(base_estimator = model1, n_estimators = n_estimators, max_samples = max_samples, random_state = seed)\n",
        "  \n",
        "  #Boosting\n",
        "  elif model_type == 'AdaBoost':\n",
        "    model1 = tree.DecisionTreeClassifier(criterion = criterion,\n",
        "            random_state = seed,max_depth = max_depth, min_samples_leaf = min_samples_leaf)\n",
        "    model = AdaBoostClassifier(base_estimator = model1, n_estimators = n_estimators, random_state = seed)\n",
        "\n",
        "  model = model.fit(X_train, t_train)\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def test_model(X_test, t_test, model, print_flag = False):\n",
        "  '''\n",
        "  Inputs: \n",
        "    X_test, t_test: Test samples and labels\n",
        "    model: A trained model.\n",
        "    print_flag: If True, the test accuracy is printed.\n",
        "\n",
        "  Output:\n",
        "    acc: Accuracy of the given model on the test data.\n",
        "  '''\n",
        "  acc = model.score(X_test, t_test)\n",
        "  \n",
        "  if print_flag:\n",
        "    print(\"Accuracy is: \", acc)\n",
        "\n",
        "  return acc\n"
      ],
      "metadata": {
        "id": "QipOlZQKeBi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting Train and Test Accuracy for different Parameter Values "
      ],
      "metadata": {
        "id": "hYv_C1ltlr__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_acc(acc_tr, acc_te, par_val, par_name, ymin = 0.75):\n",
        "  '''\n",
        "  Inputs: \n",
        "    acc_tr: Set of training accuracies (for different parameter values)\n",
        "    acc_te: Set of test accuracies\n",
        "    par_val: Set of parameter values\n",
        "    par_name (string): Name of the parameter\n",
        "    ymin: minimum value on the y-axis for the plot\n",
        "\n",
        "  Function: Generates a single plot with the train and test accuracies for different parameter values \n",
        "  '''\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  ax.plot(par_val, acc_tr, color = 'red', label = 'train')\n",
        "  ax.plot(par_val, acc_te, color = 'blue', label = 'test')\n",
        "\n",
        "  ax.legend()\n",
        "  ax.set_ylim(ymin, 1.0)\n",
        "  ax.set(xlabel = par_name, ylabel = 'Accuracy')\n",
        "  ax.set_title(('Accuracy for different values of ' + par_name))\n",
        "\n",
        "  fig.savefig('Accuracy_' + par_name + '_' + model_type + '_' + str(n_samples) + '.png')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "PCcfq-IXmMjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acc(X_tr, t_tr, X_te, t_te, par_val, par_name, model_type):\n",
        "   '''\n",
        "   Inputs: \n",
        "     X_tr, t_tr: Training samples and labels\n",
        "     X_te, t_te: Test samples and labels\n",
        "     par_val: Set of parameter values\n",
        "     par_name (string): Name of the parameter\n",
        "     model_type (string): The type of model being used, can be 'DT' for decision tree, 'RF' for random forest\n",
        "\n",
        "   Outputs: \n",
        "     acc_tr: Set of training accuracies (for different parameter values)\n",
        "     acc_te: Set of test accuracies\n",
        "   '''\n",
        "   acc_tr = []\n",
        "   acc_te = []\n",
        "   \n",
        "   for i in par_val:\n",
        "\n",
        "     if par_name == 'max_depth':\n",
        "       model = train_model(X_tr, t_tr, model_type = model_type, max_depth = i, min_samples_leaf = 1) \n",
        "\n",
        "     elif par_name == 'min_samples_leaf':\n",
        "       model = train_model(X_tr, t_tr, model_type = model_type, min_samples_leaf = i, max_depth = 10)\n",
        "\n",
        "     elif par_name == 'n_estimators':\n",
        "       model = train_model(X_tr, t_tr, model_type = model_type, n_estimators = i)\n",
        "\n",
        "     elif par_name == 'max_features':\n",
        "       model = train_model(X_tr, t_tr, model_type = model_type, max_features = i)\n",
        "\n",
        "     elif par_name == 'max_samples':\n",
        "       model = train_model(X_tr, t_tr, model_type = model_type, max_samples = i)\n",
        "\n",
        "     ac_tr = test_model(X_tr, t_tr, model)\n",
        "     ac_te = test_model(X_te, t_te, model)\n",
        "\n",
        "     acc_tr.append(ac_tr)\n",
        "     acc_te.append(ac_te)\n",
        "\n",
        "   return np.array(acc_tr), np.array(acc_te)\n"
      ],
      "metadata": {
        "id": "4Iz37ZOqtEGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_behaviour(X_tr, X_te, y_tr, y_te, vals, name, model_type, ylim = 0.75):\n",
        "  # get train and test accuracies for different parameter values\n",
        "  acc_tr, acc_te = get_acc(X_tr, y_tr, X_te, y_te, par_val=vals, par_name=name, model_type = model_type)\n",
        "  # plot the train and test accuracies \n",
        "  plot_acc(acc_tr, acc_te, vals, name, ylim)\n",
        "  "
      ],
      "metadata": {
        "id": "J5oD1orf9Dum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get train and test data"
      ],
      "metadata": {
        "id": "ady0ndx59NmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test data with 4000 samples each (Used for Parts 1-5)\n",
        "n_samples = 8000\n",
        "X_tr, X_te, y_tr8, y_te8 = get_data(n_samples = n_samples)\n",
        "\n",
        "#flatten the images into 1D inputs for tree-based methods\n",
        "X_tr8 = X_tr.reshape((n_samples//2, -1)) \n",
        "X_te8 = X_te.reshape((n_samples//2, -1))"
      ],
      "metadata": {
        "id": "bYubedZo9N0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test data with 1000 samples each (Used for Parts 3-5)\n",
        "n_samples = 2000\n",
        "X_tr, X_te, y_tr2, y_te2 = get_data(n_samples = n_samples)\n",
        "\n",
        "#flatten the images into 1D inputs for tree-based methods\n",
        "X_tr2 = X_tr.reshape((n_samples//2, -1))\n",
        "X_te2 = X_te.reshape((n_samples//2, -1))"
      ],
      "metadata": {
        "id": "MHhJnhoGwPAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot images of digits from the dataset\n",
        "\n",
        "def plot_dataset_digits(X_tr, y_tr):\n",
        "  fig = plt.figure(figsize=(7, 8))\n",
        "  columns = 3\n",
        "  rows = 3\n",
        "\n",
        "  ax = []\n",
        "\n",
        "  for i in range(columns * rows):\n",
        "    img = X_tr[2*i+1].reshape((28,28))\n",
        "    label = y_tr[2*i+1]\n",
        "    \n",
        "    ax.append(fig.add_subplot(rows, columns, i + 1))\n",
        "    ax[-1].set_title(\"Label: \" + str(label))  \n",
        "    plt.imshow(img, cmap='gray', vmin=np.min(X_tr), vmax=np.max(X_tr))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "  plt.show()  \n",
        "\n",
        "plot_dataset_digits(X_tr8, y_tr8)"
      ],
      "metadata": {
        "id": "XkT2uaLZMXdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the effect of different hyperparameters \n",
        "(Parts 1-5)"
      ],
      "metadata": {
        "id": "CLuxqZM9-GB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 1: Effect of max_depth\n",
        "\n",
        "model_type = 'DT'\n",
        "name = 'max_depth'\n",
        "vals = [1, 3, 5, 10, 15, 20]\n",
        "check_behaviour(X_tr8, X_te8, y_tr8, y_te8, vals, name, model_type, ylim = 0.7)\n",
        "\n",
        "model_type = 'RF'\n",
        "name = 'max_depth'\n",
        "vals = [1, 3, 5, 10, 15, 20]\n",
        "check_behaviour(X_tr8, X_te8, y_tr8, y_te8, vals, name, model_type, ylim = 0.7)\n"
      ],
      "metadata": {
        "id": "9v-h7FaarcgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: Effect of n_estimators\n",
        "\n",
        "model_type = 'RF'\n",
        "name = 'n_estimators'\n",
        "vals = [1, 10, 25, 50, 75, 100, 200] \n",
        "check_behaviour(X_tr8, X_te8, y_tr8, y_te8, vals, name, model_type)"
      ],
      "metadata": {
        "id": "-is876nuONyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Effect of min_samples_leaf\n",
        "\n",
        "model_type = 'RF'\n",
        "name = 'min_samples_leaf'\n",
        "vals = [1, 2, 4, 6, 8, 10, 20, 30, 40, 50] \n",
        "check_behaviour(X_tr8, X_te8, y_tr8, y_te8, vals, name, model_type)\n",
        "\n",
        "name = 'min_samples_leaf'\n",
        "vals = [1, 2, 4, 6, 8, 10, 20, 30, 40, 50] \n",
        "check_behaviour(X_tr2, X_te2, y_tr2, y_te2, vals, name, model_type)"
      ],
      "metadata": {
        "id": "R6PMc-tSOmpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: Effect of max_samples\n",
        "\n",
        "model_type = 'RF'\n",
        "name = 'max_samples'\n",
        "vals = [0.1, 0.25, 0.5, 0.75, 1.0] \n",
        "check_behaviour(X_tr8, X_te8, y_tr8, y_te8, vals, name, model_type)\n",
        "\n",
        "name = 'max_samples'\n",
        "vals = [0.1, 0.25, 0.5, 0.75, 1.0] \n",
        "check_behaviour(X_tr2, X_te2, y_tr2, y_te2, vals, name, model_type)"
      ],
      "metadata": {
        "id": "6Z_FQgdkuIko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 5: Effect of max_features\n",
        "\n",
        "model_type = 'RF'\n",
        "name = 'max_features'\n",
        "vals = [1, 10, 15, 20, 25, 30, 50, 100, 250, 500] \n",
        "check_behaviour(X_tr8, X_te8, y_tr8, y_te8, vals, name, model_type)\n",
        "\n",
        "name = 'max_features'\n",
        "vals = [1, 10, 15, 20, 25, 30, 50, 100, 250, 500] \n",
        "check_behaviour(X_tr2, X_te2, y_tr2, y_te2, vals, name, model_type)"
      ],
      "metadata": {
        "id": "h6F7A7EHEQE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize Feature Importances\n",
        "\n",
        "(Part 6) Visualize the feature importances of a random forest model trained for the binary classification task on the MNIST dataset. "
      ],
      "metadata": {
        "id": "hYjnjDG0OgRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 6\n",
        "# Based on https://github.com/probml/pyprobml/blob/master/notebooks/book1/18/rf_feature_importance_mnist.ipynb\n",
        "\n",
        "def plot_digit(data):\n",
        "    image = data.reshape(28, 28)\n",
        "    plt.figure()\n",
        "    plt.imshow(image, cmap = mpl.cm.hot,\n",
        "               interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# Train a random forest model for binary classification on MNIST, with a fixed set of parameter values\n",
        "model = train_model(X_tr8, y_tr8, model_type = 'RF')\n",
        "test_model(X_tr8, y_tr8, model)\n",
        "test_model(X_te8, y_te8, model)\n",
        "\n",
        "# Plot the feature importances\n",
        "plot_digit(model.feature_importances_)\n",
        "cbar = plt.colorbar(ticks=[model.feature_importances_.min(), model.feature_importances_.max()])\n",
        "cbar.ax.set_yticklabels(['Least important', 'Most important'])\n",
        "plt.savefig(\"rf_feature_importance_mnist.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bk2nfqKqOfcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}