{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Fashion MNIST classification (cont.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbVhjPpzn6BM"
      },
      "source": [
        "This notebook is built upon the [TensorFlow image classification tutorial](https://www.tensorflow.org/tutorials/keras/classification), which trains a neural network model to classify Fashion MNIST dataset. It's okay if you don't understand all the details of the implementation; this is a fast-paced overview of a complete TensorFlow program with the details explained as you go. We use [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-05T02:23:03.661948Z",
          "iopub.status.busy": "2022-02-05T02:23:03.661380Z",
          "iopub.status.idle": "2022-02-05T02:23:06.352001Z",
          "shell.execute_reply": "2022-02-05T02:23:06.352451Z"
        },
        "id": "dzLKpmZICaWN"
      },
      "outputs": [],
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# set random seeds\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "tf.compat.v1.set_random_seed(42)\n",
        "tf.experimental.numpy.random.seed(42)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the data as in the starter code"
      ],
      "metadata": {
        "id": "Qijn5hl3_AuU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "### Import the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLdCchMdCaWQ"
      },
      "source": [
        "You can access the Fashion MNIST directly from TensorFlow. Import and load the Fashion MNIST data directly from TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-05T02:23:06.357055Z",
          "iopub.status.busy": "2022-02-05T02:23:06.356498Z",
          "iopub.status.idle": "2022-02-05T02:23:08.243268Z",
          "shell.execute_reply": "2022-02-05T02:23:08.243667Z"
        },
        "id": "7MqDQO0KCaWS"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9FDsUlxCaWW"
      },
      "source": [
        "Loading the dataset returns four NumPy arrays:\n",
        "\n",
        "* The `train_images` and `train_labels` arrays are the *training set*—the data the model uses to learn.\n",
        "* The model is tested against the *test set*, the `test_images`, and `test_labels` arrays.\n",
        "\n",
        "We will split a validation set from the training set later."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scale and flatten the images"
      ],
      "metadata": {
        "id": "VWus4f1KIQF_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz7l27Lz9S1P"
      },
      "source": [
        "The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. Scale these values to a range of 0 to 1 before feeding them to the neural network model. To do so, divide the values by 255. Moreover, flatten the 28x28 NumPy arrays into 784 dimension feature vectors. \n",
        "\n",
        "It's important that the *training set* and the *testing set* be preprocessed in the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-05T02:23:08.495092Z",
          "iopub.status.busy": "2022-02-05T02:23:08.494541Z",
          "iopub.status.idle": "2022-02-05T02:23:08.649262Z",
          "shell.execute_reply": "2022-02-05T02:23:08.649716Z"
        },
        "id": "bW5WzIPlCaWv"
      },
      "outputs": [],
      "source": [
        "train_images = (train_images / 255.0).reshape(train_images.shape[0], -1)\n",
        "test_images = (test_images / 255.0).reshape(test_images.shape[0], -1)\n",
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *labels* are an array of integers, ranging from 0 to 9. Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"
      ],
      "metadata": {
        "id": "5h6BCgHlGwVd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-05T02:23:08.248025Z",
          "iopub.status.busy": "2022-02-05T02:23:08.247468Z",
          "iopub.status.idle": "2022-02-05T02:23:08.249506Z",
          "shell.execute_reply": "2022-02-05T02:23:08.249073Z"
        },
        "id": "IjnLH5S2CaWx"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split train/validation/test sets"
      ],
      "metadata": {
        "id": "sNQwTCz-Irog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same as the starter code, take the first 10k data in the *training set* as the training data, and the last 2k data in the *training set* as the validation data. The test data is all the 10k data in the *testing set*."
      ],
      "metadata": {
        "id": "CWsGiBrTJZNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample 5k training, 1k validation, making sure the label is balanced\n",
        "label_idxs = [np.where(train_labels == i)[0] for i in range(10)]\n",
        "Xtrain, Ytrain, Xval, Yval = \\\n",
        "    np.concatenate([train_images[label_idxs[i][:1000]] for i in range(10)]), \\\n",
        "    np.concatenate([train_labels[label_idxs[i][:1000]] for i in range(10)]), \\\n",
        "    np.concatenate([train_images[label_idxs[i][-200:]] for i in range(10)]), \\\n",
        "    np.concatenate([train_labels[label_idxs[i][-200:]] for i in range(10)])\n",
        "\n",
        "# shuffle data before training\n",
        "idx_order = np.random.permutation(Xtrain.shape[0])\n",
        "Xtrain = Xtrain[idx_order]\n",
        "Ytrain = Ytrain[idx_order]\n",
        "\n",
        "Xtest, Ytest = test_images, test_labels"
      ],
      "metadata": {
        "id": "E3F8XZwbIyTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee638AlnCaWz"
      },
      "source": [
        "To verify that the data is in the correct format and that you're ready to build and train the network, let's display the first 25 images from the training data and display the class name below each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-05T02:23:08.719166Z",
          "iopub.status.busy": "2022-02-05T02:23:08.718438Z",
          "iopub.status.idle": "2022-02-05T02:23:09.508634Z",
          "shell.execute_reply": "2022-02-05T02:23:09.509037Z"
        },
        "id": "oZTImqg_CaW1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(Xtrain[i].reshape(28, 28), cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[Ytrain[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59veuiEZCaW4"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Building the neural network requires configuring the layers of the model, then compiling the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxg1XGm0eOBy"
      },
      "source": [
        "### Set up two linear layers\n",
        "\n",
        "The basic building block of a neural network is the [*layer*](https://www.tensorflow.org/api_docs/python/tf/keras/layers). Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand.\n",
        "\n",
        "Most of deep learning consists of chaining together simple layers. Most layers, such as `tf.keras.layers.Dense`, have parameters that are learned during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-05T02:23:09.515540Z",
          "iopub.status.busy": "2022-02-05T02:23:09.514954Z",
          "iopub.status.idle": "2022-02-05T02:23:11.016295Z",
          "shell.execute_reply": "2022-02-05T02:23:11.015745Z"
        },
        "id": "9ODch-OFCaW4"
      },
      "outputs": [],
      "source": [
        "base_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gut8A_7rCaW6"
      },
      "source": [
        "The network consists of a sequence of two `tf.keras.layers.Dense` layers. These are densely connected, or fully connected, neural layers. The first `Dense` layer has 1000 nodes (or neurons). The second layer returns a logits array with length of 10. Each node contains a score that indicates the current image belongs to one of the 10 classes.\n",
        "\n",
        "### Compile the model\n",
        "\n",
        "Before the model is ready for training, it needs a few more settings. These are added during the model's [*compile*](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) step:\n",
        "\n",
        "* [*Loss function*](https://www.tensorflow.org/api_docs/python/tf/keras/losses) —This measures how accurate the model is during training. You want to minimize this function to \"steer\" the model in the right direction.\n",
        "* [*Optimizer*](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) —This is how the model is updated based on the data it sees and its loss function.\n",
        "* [*Metrics*](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) —Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-05T02:23:11.023505Z",
          "iopub.status.busy": "2022-02-05T02:23:11.022906Z",
          "iopub.status.idle": "2022-02-05T02:23:11.028191Z",
          "shell.execute_reply": "2022-02-05T02:23:11.027758Z"
        },
        "id": "Lhan11blCaW7"
      },
      "outputs": [],
      "source": [
        "base_model.compile(optimizer='sgd',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup early-stopping\n",
        "\n",
        "Define the early-stopping module as a callback module to stop the training process."
      ],
      "metadata": {
        "id": "8CR10hLFV6QB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        # Stop training when `val_loss` is no longer improving\n",
        "        monitor=\"val_accuracy\",\n",
        "        # \"no longer improving\" being defined as \"for at least 3 epochs\"\n",
        "        patience=3,\n",
        "        verbose=1,\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "KiIiCEvHV404"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKF6uW-BCaW-"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Training the neural network model requires the following steps:\n",
        "\n",
        "1. Feed the training data to the model. In this example, the training data is in the `Xtrain` and `Ytrain` arrays.\n",
        "2. The model learns to associate images and labels.\n",
        "3. You ask the model to make predictions about a test set—in this example, the `Xtest` array.\n",
        "4. Verify that the predictions match the labels from the `Ytest` array.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4P4zIV7E28Z"
      },
      "source": [
        "### Feed the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To start training,  call the [`model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) method—so called because it \"fits\" the model to the training data. It automatically processes the forward pass and the backward pass.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iOuzJQK4WBNY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-05T02:23:11.032563Z",
          "iopub.status.busy": "2022-02-05T02:23:11.031996Z",
          "iopub.status.idle": "2022-02-05T02:23:45.127248Z",
          "shell.execute_reply": "2022-02-05T02:23:45.126760Z"
        },
        "id": "xvwvpA64CaW_"
      },
      "outputs": [],
      "source": [
        "base_model.fit(\n",
        "    Xtrain, Ytrain, \n",
        "    batch_size=5, epochs=50, \n",
        "    callbacks=callbacks,\n",
        "    validation_data=(Xval, Yval))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCpr6DGyE28h"
      },
      "source": [
        "### Evaluate accuracy\n",
        "\n",
        "Next, compare how the model performs on the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-02-05T02:23:45.131545Z",
          "iopub.status.busy": "2022-02-05T02:23:45.130989Z",
          "iopub.status.idle": "2022-02-05T02:23:45.773332Z",
          "shell.execute_reply": "2022-02-05T02:23:45.773676Z"
        },
        "id": "VflXLEeECaXC"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = base_model.evaluate(Xtest,  Ytest, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The accuracy that we obtain here is similar (+/- 1.5) to the accuracy from your own Python code for a batch size of 5. You can check your results to verify this.**"
      ],
      "metadata": {
        "id": "7GidnVPiQx48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3.3: The effect of early-stopping\n",
        "\n",
        "We provided built-in early-stopping in the starter code, but did not explore the effect that it has on training. In this question, we will explore the effect of early-stopping. To do this, we evaluate the training, validation and test accuracy for each training epoch. We then plot the training, validation and test accuracy throughout the training process to see how early-stopping works.\n",
        "\n",
        "To make sure the training process is the same as the previous `base_model`, we set the same random seed for training. Connecting all the training-evaluation modules together, we train 30 epochs on a batch size of 5 without early-stopping:"
      ],
      "metadata": {
        "id": "REe14FxQR0hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the same random seed to make sure the model initialized at the same weights.\n",
        "tf.random.set_seed(42)\n",
        "tf.compat.v1.set_random_seed(42)\n",
        "tf.experimental.numpy.random.seed(42)\n",
        "# build the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "test_acc = []\n",
        "# setup a callback for evaluate the test set\n",
        "class TestCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        res_test = self.model.evaluate(Xtest, Ytest, verbose = 0)\n",
        "        test_acc.append(res_test[1])\n",
        "my_test_callback = TestCallback()\n",
        "# compile the model\n",
        "model.compile(optimizer='sgd',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "# train the model without callbacks\n",
        "train_test_history = model.fit(\n",
        "    Xtrain, Ytrain, \n",
        "    batch_size=5, epochs=30,\n",
        "    callbacks=[my_test_callback],\n",
        "    validation_data=(Xval, Yval))"
      ],
      "metadata": {
        "id": "0RUtt6lbXjqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the accuracy without early-stopping\n",
        "\n",
        "Run the provided code to plot the training accuracy *vs.* the number of epochs. On the same graph, we plot the test accuracy *vs.* the number of epochs. To do this, we retrieve loss and accuracy information in `train_test_history.history`. We also use `plt.scatter` to mark the point on the test accuracy curve where we early-stopped previously."
      ],
      "metadata": {
        "id": "WQvU2EzF0gN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(len(train_test_history.history['accuracy']))\n",
        "train_acc = train_test_history.history['accuracy']\n",
        "val_acc = train_test_history.history['val_accuracy']\n",
        "\n",
        "plt.title(\"Train-val-test accuracy through the epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.plot(x, train_acc, label=\"train acc\")\n",
        "plt.plot(x, val_acc, label=\"val acc\")\n",
        "plt.plot(x, test_acc, label=\"test acc\")\n",
        "plt.scatter([x[12]], [val_acc[12]], color=\"red\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2YmAuNylVQLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the curve, answer the following questions:\n",
        "\n",
        "*   What is the trend of training and test accuracy after the early-stopped point?\n",
        "*   Based on the plot, what do you think could go wrong if the patience parameter for early-stopping is too small?\n",
        "\n"
      ],
      "metadata": {
        "id": "pFooIqM33Jzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3.4: SGD with momentum\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2XFYhYd3zDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We mentioned in the lecture that adding a “momentum” term, which encourages model to continue along previous gradient direction, helps the network to converge. Concretely, with an initial velocity $v=0$, we update the gradient by\n",
        "\n",
        "$v ← \\alpha v + \\nabla F(w)$\n",
        "\n",
        "$w ← w - \\eta v$\n",
        "\n",
        "where $\\eta$ is the learning rate and $\\alpha$ is the factor describing how much weight we put on the previous gradients. $\\alpha=0$ is equivalent to gradient update without momentum. \n",
        "\n",
        "We provide the `sgd_with_momentum` function with argument $\\alpha$ below. In this question, we will explore the  momentum factor $\\alpha$ effects training loss and test accuracy."
      ],
      "metadata": {
        "id": "YIMw1M5A6UUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd_with_momentum(alpha):\n",
        "  # set the same random seed to make sure the model initialized at the same weights.\n",
        "  tf.random.set_seed(42)\n",
        "  tf.compat.v1.set_random_seed(42)\n",
        "  tf.experimental.numpy.random.seed(42)\n",
        "  # build the model\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "  # compile the model\n",
        "  model.compile(optimizer=tf.keras.optimizers.SGD(momentum=alpha),\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "  # train the model without early-stopping\n",
        "  history = model.fit(\n",
        "      Xtrain, Ytrain, \n",
        "      batch_size=5, epochs=30)\n",
        "  # evaluate the model\n",
        "  test_loss, test_acc = model.evaluate(Xtest,  Ytest, verbose=2)\n",
        "  # return training loss and test accuracy\n",
        "  return history.history['loss']"
      ],
      "metadata": {
        "id": "Bup9vrQY5UR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize training loss and test accuracy with different momentum factor $\\alpha$\n",
        "\n",
        "We will explore how different momentum factor $\\alpha$ effects training loss and test accuracy. Call the `sgd_with_momentum` function with $\\alpha$ = 0.1, 0.3, 0.5, 0.7, 0.9. Store the training loss and test accuracy from the return of the function."
      ],
      "metadata": {
        "id": "metYxQeR9NK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "\n",
        "for alpha in np.arange(0.1, 1, 0.2):\n",
        "  train_loss = sgd_with_momentum(alpha)\n",
        "  train_losses.append(train_loss)"
      ],
      "metadata": {
        "id": "yhoOHk66ZlmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We visualize the training loss. Plot 5 curve of different $\\alpha$ on the same graph. Each curve has epoch number as the $x$-axis and the training loss as the $y$-axis.  Based on this, what is a suitable value of $\\alpha$? Therefore, how should training ideally rely on previous gradients for better convergence?"
      ],
      "metadata": {
        "id": "ubvOBXtq_47s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(len(train_losses[0]))\n",
        "\n",
        "plt.title(\"Training loss through the epochs for different SGD momentum\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "for i, alpha in enumerate(np.arange(0.1, 1, 0.2)):\n",
        "  plt.plot(x, train_losses[i], label=\"momentum=%f\" % alpha)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dUXEDSZUTyyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Bonus] Problem 3.5: Out-of-domain generalization"
      ],
      "metadata": {
        "id": "HNMB9pN15Kfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A major research direction at present is to ensure that our ML models not only do well on test data drawn from the same distribution as training data, but also do well when they get data from distributions different from the original distribution. This is known as Out-of-Distribution (OOD) generalization. In the Fashion MNIST dataset, the training and test set are drawn from the same distribution. Let's explore how our models do on test data coming from a slightly different distribution.\n"
      ],
      "metadata": {
        "id": "DY8pm2aRCTza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create OOD test set by translation\n",
        "\n",
        "Take a look at the original test set of Fashion MNIST."
      ],
      "metadata": {
        "id": "JoIQ7FJZYKPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(Xtest[i].reshape(28, 28), cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[Ytest[i]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fz3bd7WyZv3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We move up the images of the test set by 4 pixels, and create a new translated test set from that. You will notice that to a human eye, the new test set is not any more difficult than the original test set."
      ],
      "metadata": {
        "id": "fRMYFpliIcYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lift the image by 4 pixels\n",
        "translate_x = 4\n",
        "translate_y = 0\n",
        "\n",
        "Xtest_t4 = np.pad(Xtest.reshape(-1, 28, 28)[:, translate_x:, translate_y:], \n",
        "                  ((0, 0), (0, translate_x), (0, translate_y))).reshape(-1, 784)\n",
        "\n",
        "\n",
        "# visualize the translation\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(Xtest_t4[i].reshape(28, 28), cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[Ytest[i]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cCAn57GDZT3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*But can the MLP model still identify it well?* Check the test accuracy on the base model."
      ],
      "metadata": {
        "id": "YQGGXmBQJRJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = base_model.evaluate(Xtest_t4,  Ytest, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "5sbsD36kcVo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Neural Network\n",
        "\n",
        "What if we try a different model architecture? For example, in class we saw that convolutional neural networks are good at dealing with image translation. We replace the first linear layer with a convolutional layer of 64 kernels with size $7\\times7$, followed by max-pooling. This can be done with just one or two lines of code in TensorFlow. We train the 2-layer CNN on the original training data and test it on both the original and the translated test sets."
      ],
      "metadata": {
        "id": "SsMCsh9FYHR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the same random seed.\n",
        "tf.random.set_seed(42)\n",
        "tf.compat.v1.set_random_seed(42)\n",
        "tf.experimental.numpy.random.seed(42)\n",
        "# replace the linear layer with a conv layer of 64 7*7 kernels, followed by maxpooling.\n",
        "cnn_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Reshape((28, 28, 1), input_shape=(784,)),\n",
        "    tf.keras.layers.Conv2D(64, (7, 7), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "# compile the model\n",
        "cnn_model.compile(optimizer='sgd',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "# train the model with early-stopping\n",
        "cnn_model.fit(\n",
        "    Xtrain, Ytrain, \n",
        "    batch_size=5, epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=(Xval, Yval))\n",
        "# evaluate on the original test set\n",
        "test_loss, test_acc = cnn_model.evaluate(Xtest,  Ytest, verbose=2)\n",
        "print('In-domain test accuracy:', test_acc)\n",
        "# evaluate on the translated test set\n",
        "test_loss, test_acc = cnn_model.evaluate(Xtest_t4,  Ytest, verbose=2)\n",
        "print('Out-of-domain test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "KmGDINQK9Z7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the CNN model has fewer parameters (compute by hand, you can use \n",
        "`cnn_model.trainable_variables` to verify your calculation) than the 2-layer \n",
        "MLP. What is the in-domain test accuracy and the translated test accuracy?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yzZE5z1pOA9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make the CNN Deeper\n",
        "\n",
        "Going one step further, we can make the CNN deeper by adding one more convolutional layer between the two existing layers. "
      ],
      "metadata": {
        "id": "LilCHqz5QqAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the same random seed.\n",
        "tf.random.set_seed(42)\n",
        "tf.compat.v1.set_random_seed(42)\n",
        "tf.experimental.numpy.random.seed(42)\n",
        "# add one more conv layer of 128 2*2 kernels in between, followed by maxpooling.\n",
        "cnn_model_deeper = tf.keras.Sequential([\n",
        "    tf.keras.layers.Reshape((28, 28, 1), input_shape=(784,)),\n",
        "    tf.keras.layers.Conv2D(64, (7, 7), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    tf.keras.layers.Conv2D(128, (2, 2), activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "# compile the model\n",
        "cnn_model_deeper.compile(optimizer='sgd',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "# train the model with early-stopping\n",
        "cnn_model_deeper.fit(\n",
        "    Xtrain, Ytrain, \n",
        "    batch_size=5, epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    validation_data=(Xval, Yval))\n",
        "# evaluate on the original test set\n",
        "test_loss, test_acc = cnn_model_deeper.evaluate(Xtest,  Ytest, verbose=2)\n",
        "print('In-domain test accuracy:', test_acc)\n",
        "# evaluate on the translated test set\n",
        "test_loss, test_acc = cnn_model_deeper.evaluate(Xtest_t4,  Ytest, verbose=2)\n",
        "print('Out-of-domain test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "45qQeQ15n1tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the deeper 3-layer CNN model has fewer parameters (compute by hand, you can use `cnn_model.trainable_variables` to verify your calculation) than the 2-layer CNN model. What is the in-domain and the translated test accuracy of the deeper CNN model? Provide some intuition on why it is better than the 2-layer CNN on the translated set."
      ],
      "metadata": {
        "id": "fgjkdk-jqC2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create OOD test set by rotation\n",
        "\n"
      ],
      "metadata": {
        "id": "bq3J2igvwGrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create 3 more OOD test sets by rotation. We rotate the images in the original test set by 90, 180 and 270 degrees."
      ],
      "metadata": {
        "id": "uvHTbREBSOAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rotate 90 degrees counter-clockwise \n",
        "Xtest_r90 = np.rot90(Xtest.reshape(-1, 28, 28), k=1, axes=(1, 2)).reshape(-1, 784)\n",
        "\n",
        "# rotate 180 degrees counter-clockwise \n",
        "Xtest_r180 = np.rot90(Xtest.reshape(-1, 28, 28), k=2, axes=(1, 2)).reshape(-1, 784)\n",
        "\n",
        "# rotate 270 degrees counter-clockwise \n",
        "Xtest_r270 = np.rot90(Xtest.reshape(-1, 28, 28), k=3, axes=(1, 2)).reshape(-1, 784)\n",
        "\n",
        "# visualize the 90 degrees rotation\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(Xtest_r90[i].reshape(28, 28), cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[Ytest[i]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xT_gvtHTrk-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide the test accuracy of the base model, the CNN model  and the deeper CNN model on the three rotation test sets. Are the CNN and the deeper CNN still doing well?"
      ],
      "metadata": {
        "id": "V8CL8fldwRBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rotation 90 degrees:\")\n",
        "\n",
        "test_loss, test_acc = base_model.evaluate(Xtest_r90,  Ytest, verbose=2)\n",
        "print('Base model accuracy:', test_acc)\n",
        "test_loss, test_acc = cnn_model.evaluate(Xtest_r90,  Ytest, verbose=2)\n",
        "print('CNN model accuracy:', test_acc)\n",
        "test_loss, test_acc = cnn_model_deeper.evaluate(Xtest_r90,  Ytest, verbose=2)\n",
        "print('Deeper CNN model accuracy:', test_acc)\n",
        "\n",
        "print(\"Rotation 180 degrees:\")\n",
        "\n",
        "test_loss, test_acc = base_model.evaluate(Xtest_r180,  Ytest, verbose=2)\n",
        "print('Base model accuracy:', test_acc)\n",
        "test_loss, test_acc = cnn_model.evaluate(Xtest_r180,  Ytest, verbose=2)\n",
        "print('CNN model accuracy:', test_acc)\n",
        "test_loss, test_acc = cnn_model_deeper.evaluate(Xtest_r270,  Ytest, verbose=2)\n",
        "print('Deeper CNN model accuracy:', test_acc)\n",
        "\n",
        "print(\"Rotation 270 degrees:\")\n",
        "\n",
        "test_loss, test_acc = base_model.evaluate(Xtest_r270,  Ytest, verbose=2)\n",
        "print('Base model accuracy:', test_acc)\n",
        "test_loss, test_acc = cnn_model.evaluate(Xtest_r270,  Ytest, verbose=2)\n",
        "print('CNN model accuracy:', test_acc)\n",
        "test_loss, test_acc = cnn_model_deeper.evaluate(Xtest_r270,  Ytest, verbose=2)\n",
        "print('Deeper CNN model accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "WOq3a0I2sj3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}